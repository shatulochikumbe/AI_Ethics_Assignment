Case study answers 

Case 1: Amazon’s Biased Hiring Tool
Source of Bias: Historical hiring data reflecting past gender discrimination (male-dominated tech hires).

Proposed Fixes:

Debiasing Training Data: Oversample underrepresented groups or use synthetic data augmentation.

Adversarial Debiasing: Implement a discriminator network to penalize gender-correlated features during training.

Human-in-the-Loop: Use AI as a screening assistant rather than autonomous decision-maker, with human reviewers for final decisions.

Fairness Metrics:

Disparate Impact Ratio (should be ≥0.8)

Equal Opportunity Difference (should approach zero)

Demographic Parity in shortlisted candidates

Case 2: Facial Recognition in Policing
Ethical Risks:

Wrongful Arrests: Higher false positive rates for minorities lead to unjust detentions.

Privacy Erosion: Mass surveillance without consent violates civil liberties.

Feedback Loops: Over-policing in minority communities generates biased training data.

Policy Recommendations:

Accuracy Requirements: Minimum 99% accuracy across all demographic groups before deployment.

Independent Audits: Third-party fairness assessments with public reports.

Use Limitations: Restrict to investigative leads only, never as sole evidence for arrests.

Community Oversight: Citizen review boards for deployment approvals.





